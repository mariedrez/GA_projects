{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df59029c",
   "metadata": {},
   "source": [
    "\n",
    "# Part 1: Webscraping & Data Collection\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2bb251",
   "metadata": {},
   "source": [
    "## (1) Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f59dc030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime as dt \n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965bc10",
   "metadata": {},
   "source": [
    "## (2) Webscraping\n",
    "\n",
    "Access to Reddit data on its subreddit forums must be gained before any analysis can be done. This was achieved with Pushshift API, which allows us to search Reddit data easily. For this project, the main endpoint used to search all publicly available submissions on Reddit is \"/reddit/search/submission\". The following function titled 'scrape' will be used to restrict scraping from a named subreddit, for a set number of posts which were made within the past n number of days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3036127",
   "metadata": {},
   "source": [
    "### Define function for webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df3deace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(subreddit, n, days = 30):\n",
    "    \n",
    "    # Url\n",
    "    base_url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    full_url = f'{base_url}?subreddit={subreddit}&size=100'\n",
    "    \n",
    "    # Create an empty list to store posts\n",
    "    posts = []\n",
    "    \n",
    "    # Modify the url after each iteration\n",
    "    for i in range(1, n+1):\n",
    "        urlmod = '{}&after={}d'.format(full_url, days*i)\n",
    "        res_1 = requests.get(urlmod)\n",
    "        \n",
    "        # This is to prevent errors from stopping the codes from running\n",
    "        try:\n",
    "            res = requests.get(urlmod)\n",
    "            assert res.status_code == 200\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # Convert data to json object\n",
    "        extracted = res.json()['data']\n",
    "        \n",
    "        # Construct a Pandas dataframe from dict\n",
    "        df = pd.DataFrame.from_dict(extracted)\n",
    "        \n",
    "        # Add the df to 'posts' list\n",
    "        posts.append(df)\n",
    "        \n",
    "        # Total number of posts scraped\n",
    "        total_scraped = sum(len(x) for x in posts)\n",
    "        \n",
    "        # Scrape only specific n amount of data \n",
    "        if total_scraped > n:\n",
    "            break\n",
    "        \n",
    "        # Generate a random sleep duration to simulate a human user\n",
    "        sleep_duration = random.randint(1,9)\n",
    "        time.sleep(sleep_duration)\n",
    "            \n",
    "    \n",
    "    # Create a list of features of interest \n",
    "    features_of_interest = ['subreddit', 'title', 'selftext']\n",
    "    \n",
    "    # Combine all iterations into 1 dataframe\n",
    "    final_df = pd.concat(posts, sort=False)\n",
    "    # And remove all the unrequired columns from the datasets\n",
    "    final_df = final_df[features_of_interest]\n",
    "    # Drop any duplicates\n",
    "    final_df.drop_duplicates(inplace=True)\n",
    "    return final_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3628e72d",
   "metadata": {},
   "source": [
    "### Scrape data & Store as df\n",
    "Pass the function to scrape data from each subreddit r/Anxiety and r/depression, and assign the data to its own respective variable <i>submissions_anxiety_df</i> and <i>submissions_depression_df</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c710775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3069 submissions on 'Anxiety' from Pushshift\n",
      "Retrieved 3075 submissions on 'depression' from Pushshift\n"
     ]
    }
   ],
   "source": [
    "submissions_anxiety_df = scrape('Anxiety', 3000, days = 30)\n",
    "submissions_depression_df = scrape('depression', 3000, days = 30)\n",
    "\n",
    "print(f'Retrieved {len(submissions_anxiety_df)} submissions on \\'Anxiety\\' from Pushshift')\n",
    "print((f'Retrieved {len(submissions_depression_df)} submissions on \\'depression\\' from Pushshift'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5118e3ee",
   "metadata": {},
   "source": [
    "## (3) Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd9c338",
   "metadata": {},
   "source": [
    "### Export as csv\n",
    "Raw data collected from the r/Anxiety and r/depression subreddits are saved and exported via separate csv files.\n",
    "Specify <i>index=False</i> to avoid the addition of an unnamed column or additional index when these csv files are read in again in later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18043e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_anxiety_df.to_csv('./data/anxiety_data.csv', index=False)\n",
    "submissions_depression_df.to_csv('./data/depression_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
