{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc750890",
   "metadata": {},
   "source": [
    "# Part 3: Modelling, Evaluation and Conclusion\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86215366",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e21fec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b47e6",
   "metadata": {},
   "source": [
    "### Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8651fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframes\n",
    "anxiety_cleaned = pd.read_csv('./data/anxiety_cleaned.csv')\n",
    "depression_cleaned = pd.read_csv('./data/depression_cleaned.csv')\n",
    "both = pd.read_csv('./data/raw_cleaned.csv') # combined both raw data collected and cleaned data at each stage for both subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed4b6c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post</th>\n",
       "      <th>post_cleaned</th>\n",
       "      <th>post_tokenised</th>\n",
       "      <th>post_no_stop</th>\n",
       "      <th>post_lemmatised</th>\n",
       "      <th>post_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>Coping We’re moving out of state and away from...</td>\n",
       "      <td>Coping We’re moving out of state and away from...</td>\n",
       "      <td>['coping', 'we', 're', 'moving', 'out', 'of', ...</td>\n",
       "      <td>['coping', 'moving', 'state', 'away', 'everyon...</td>\n",
       "      <td>coping moving state away everyone know sunday ...</td>\n",
       "      <td>coping moving state away everyone know sunday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>Starting a new job anxiety As the title says, ...</td>\n",
       "      <td>Starting a new job anxiety As the title says I...</td>\n",
       "      <td>['starting', 'a', 'new', 'job', 'anxiety', 'as...</td>\n",
       "      <td>['starting', 'new', 'job', 'anxiety', 'title',...</td>\n",
       "      <td>starting new job anxiety title say starting ne...</td>\n",
       "      <td>starting new job anxiety title say starting ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>anxiety affecting studies Lately I've been fee...</td>\n",
       "      <td>anxiety affecting studies Lately Ive been feel...</td>\n",
       "      <td>['anxiety', 'affecting', 'studies', 'lately', ...</td>\n",
       "      <td>['anxiety', 'affecting', 'studies', 'lately', ...</td>\n",
       "      <td>anxiety affecting study lately feeling anxious...</td>\n",
       "      <td>anxiety affecting study lately feeling anxious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>This is my recovery story since falling for th...</td>\n",
       "      <td>This is my recovery story since falling for th...</td>\n",
       "      <td>['this', 'is', 'my', 'recovery', 'story', 'sin...</td>\n",
       "      <td>['recovery', 'story', 'since', 'falling', 'pin...</td>\n",
       "      <td>recovery story since falling pin code scam cam...</td>\n",
       "      <td>recovery story since falling pin code scam cam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>After years wearing braces my teeth still look...</td>\n",
       "      <td>After years wearing braces my teeth still look...</td>\n",
       "      <td>['after', 'years', 'wearing', 'braces', 'my', ...</td>\n",
       "      <td>['years', 'wearing', 'braces', 'teeth', 'still...</td>\n",
       "      <td>year wearing brace teeth still look fucked dup...</td>\n",
       "      <td>year wearing brace teeth still look fucked dup...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit                                               post  \\\n",
       "0   Anxiety  Coping We’re moving out of state and away from...   \n",
       "1   Anxiety  Starting a new job anxiety As the title says, ...   \n",
       "2   Anxiety  anxiety affecting studies Lately I've been fee...   \n",
       "3   Anxiety  This is my recovery story since falling for th...   \n",
       "4   Anxiety  After years wearing braces my teeth still look...   \n",
       "\n",
       "                                        post_cleaned  \\\n",
       "0  Coping We’re moving out of state and away from...   \n",
       "1  Starting a new job anxiety As the title says I...   \n",
       "2  anxiety affecting studies Lately Ive been feel...   \n",
       "3  This is my recovery story since falling for th...   \n",
       "4  After years wearing braces my teeth still look...   \n",
       "\n",
       "                                      post_tokenised  \\\n",
       "0  ['coping', 'we', 're', 'moving', 'out', 'of', ...   \n",
       "1  ['starting', 'a', 'new', 'job', 'anxiety', 'as...   \n",
       "2  ['anxiety', 'affecting', 'studies', 'lately', ...   \n",
       "3  ['this', 'is', 'my', 'recovery', 'story', 'sin...   \n",
       "4  ['after', 'years', 'wearing', 'braces', 'my', ...   \n",
       "\n",
       "                                        post_no_stop  \\\n",
       "0  ['coping', 'moving', 'state', 'away', 'everyon...   \n",
       "1  ['starting', 'new', 'job', 'anxiety', 'title',...   \n",
       "2  ['anxiety', 'affecting', 'studies', 'lately', ...   \n",
       "3  ['recovery', 'story', 'since', 'falling', 'pin...   \n",
       "4  ['years', 'wearing', 'braces', 'teeth', 'still...   \n",
       "\n",
       "                                     post_lemmatised  \\\n",
       "0  coping moving state away everyone know sunday ...   \n",
       "1  starting new job anxiety title say starting ne...   \n",
       "2  anxiety affecting study lately feeling anxious...   \n",
       "3  recovery story since falling pin code scam cam...   \n",
       "4  year wearing brace teeth still look fucked dup...   \n",
       "\n",
       "                                         post_string  \n",
       "0  coping moving state away everyone know sunday ...  \n",
       "1  starting new job anxiety title say starting ne...  \n",
       "2  anxiety affecting study lately feeling anxious...  \n",
       "3  recovery story since falling pin code scam cam...  \n",
       "4  year wearing brace teeth still look fucked dup...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post</th>\n",
       "      <th>post_cleaned</th>\n",
       "      <th>post_tokenised</th>\n",
       "      <th>post_no_stop</th>\n",
       "      <th>post_lemmatised</th>\n",
       "      <th>post_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>depression</td>\n",
       "      <td>Lack of cooperation from people: being left ou...</td>\n",
       "      <td>Lack of cooperation from people being left out...</td>\n",
       "      <td>['lack', 'of', 'cooperation', 'from', 'people'...</td>\n",
       "      <td>['lack', 'cooperation', 'people', 'left', 'peo...</td>\n",
       "      <td>lack cooperation people left people want get k...</td>\n",
       "      <td>lack cooperation people left people want get k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>depression</td>\n",
       "      <td>I hate to sleep I can’t sleep, and I haven’t b...</td>\n",
       "      <td>I hate to sleep I can’t sleep and I haven’t be...</td>\n",
       "      <td>['i', 'hate', 'to', 'sleep', 'i', 'can', 't', ...</td>\n",
       "      <td>['hate', 'sleep', 'sleep', 'sleeping', 'long',...</td>\n",
       "      <td>hate sleep sleep sleeping long time night trie...</td>\n",
       "      <td>hate sleep sleep sleeping long time night trie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>depression</td>\n",
       "      <td>Just another day folks! Let’s fucking wrap it ...</td>\n",
       "      <td>Just another day folks Let’s fucking wrap it u...</td>\n",
       "      <td>['just', 'another', 'day', 'folks', 'let', 's'...</td>\n",
       "      <td>['another', 'day', 'folks', 'let', 'fucking', ...</td>\n",
       "      <td>another day folk let fucking wrap hour fuck</td>\n",
       "      <td>another day folk let fucking wrap hour fuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>depression</td>\n",
       "      <td>My house is a sad place It’s been made for bus...</td>\n",
       "      <td>My house is a sad place It’s been made for bus...</td>\n",
       "      <td>['my', 'house', 'is', 'a', 'sad', 'place', 'it...</td>\n",
       "      <td>['house', 'sad', 'place', 'made', 'business', ...</td>\n",
       "      <td>house sad place made business efficiency leavi...</td>\n",
       "      <td>house sad place made business efficiency leavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>depression</td>\n",
       "      <td>I just want to die I just hate my life. I need...</td>\n",
       "      <td>I just want to die I just hate my life I need ...</td>\n",
       "      <td>['i', 'just', 'want', 'to', 'die', 'i', 'just'...</td>\n",
       "      <td>['want', 'die', 'hate', 'life', 'need', 'numb'...</td>\n",
       "      <td>want die hate life need numb act constantly ke...</td>\n",
       "      <td>want die hate life need numb act constantly ke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit                                               post  \\\n",
       "0  depression  Lack of cooperation from people: being left ou...   \n",
       "1  depression  I hate to sleep I can’t sleep, and I haven’t b...   \n",
       "2  depression  Just another day folks! Let’s fucking wrap it ...   \n",
       "3  depression  My house is a sad place It’s been made for bus...   \n",
       "4  depression  I just want to die I just hate my life. I need...   \n",
       "\n",
       "                                        post_cleaned  \\\n",
       "0  Lack of cooperation from people being left out...   \n",
       "1  I hate to sleep I can’t sleep and I haven’t be...   \n",
       "2  Just another day folks Let’s fucking wrap it u...   \n",
       "3  My house is a sad place It’s been made for bus...   \n",
       "4  I just want to die I just hate my life I need ...   \n",
       "\n",
       "                                      post_tokenised  \\\n",
       "0  ['lack', 'of', 'cooperation', 'from', 'people'...   \n",
       "1  ['i', 'hate', 'to', 'sleep', 'i', 'can', 't', ...   \n",
       "2  ['just', 'another', 'day', 'folks', 'let', 's'...   \n",
       "3  ['my', 'house', 'is', 'a', 'sad', 'place', 'it...   \n",
       "4  ['i', 'just', 'want', 'to', 'die', 'i', 'just'...   \n",
       "\n",
       "                                        post_no_stop  \\\n",
       "0  ['lack', 'cooperation', 'people', 'left', 'peo...   \n",
       "1  ['hate', 'sleep', 'sleep', 'sleeping', 'long',...   \n",
       "2  ['another', 'day', 'folks', 'let', 'fucking', ...   \n",
       "3  ['house', 'sad', 'place', 'made', 'business', ...   \n",
       "4  ['want', 'die', 'hate', 'life', 'need', 'numb'...   \n",
       "\n",
       "                                     post_lemmatised  \\\n",
       "0  lack cooperation people left people want get k...   \n",
       "1  hate sleep sleep sleeping long time night trie...   \n",
       "2        another day folk let fucking wrap hour fuck   \n",
       "3  house sad place made business efficiency leavi...   \n",
       "4  want die hate life need numb act constantly ke...   \n",
       "\n",
       "                                         post_string  \n",
       "0  lack cooperation people left people want get k...  \n",
       "1  hate sleep sleep sleeping long time night trie...  \n",
       "2        another day folk let fucking wrap hour fuck  \n",
       "3  house sad place made business efficiency leavi...  \n",
       "4  want die hate life need numb act constantly ke...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View top rows of separate cleaned datasets\n",
    "display(anxiety_cleaned.head())\n",
    "display(depression_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c649d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the 'anxiety_cleaned' and 'depression_cleaned' dataset\n",
    "combined_cleaned = pd.concat([anxiety_cleaned, depression_cleaned])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dbd6ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"combined_cleaned\" dataset:\n",
      "(Rows, Column): (5337, 7)\n",
      "\n",
      "\"both\" dataset:\n",
      "(Rows, Column): (5337, 6)\n"
     ]
    }
   ],
   "source": [
    "# Check the number of rows and columns in the cleaned dataset\n",
    "print(\"\\\"combined_cleaned\\\" dataset:\")\n",
    "print(f\"(Rows, Column): {combined_cleaned.shape}\")\n",
    "print(\"\")\n",
    "# Check the number of rows and columns in the raw dataset\n",
    "print(\"\\\"both\\\" dataset:\")\n",
    "print(f\"(Rows, Column): {both.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef857c",
   "metadata": {},
   "source": [
    "The total number of rows is 5337. As the newly merged dataframe with cleaned datasets ('combined_cleaned') contain same total number of rows as the merged dataframe with raw datasets ('raw_cleaned'), this means that there has been no loss of data from the cleaning and pre-processing phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55143b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post</th>\n",
       "      <th>post_cleaned</th>\n",
       "      <th>post_tokenised</th>\n",
       "      <th>post_no_stop</th>\n",
       "      <th>post_lemmatised</th>\n",
       "      <th>post_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>Coping We’re moving out of state and away from...</td>\n",
       "      <td>Coping We’re moving out of state and away from...</td>\n",
       "      <td>['coping', 'we', 're', 'moving', 'out', 'of', ...</td>\n",
       "      <td>['coping', 'moving', 'state', 'away', 'everyon...</td>\n",
       "      <td>coping moving state away everyone know sunday ...</td>\n",
       "      <td>coping moving state away everyone know sunday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>Starting a new job anxiety As the title says, ...</td>\n",
       "      <td>Starting a new job anxiety As the title says I...</td>\n",
       "      <td>['starting', 'a', 'new', 'job', 'anxiety', 'as...</td>\n",
       "      <td>['starting', 'new', 'job', 'anxiety', 'title',...</td>\n",
       "      <td>starting new job anxiety title say starting ne...</td>\n",
       "      <td>starting new job anxiety title say starting ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>anxiety affecting studies Lately I've been fee...</td>\n",
       "      <td>anxiety affecting studies Lately Ive been feel...</td>\n",
       "      <td>['anxiety', 'affecting', 'studies', 'lately', ...</td>\n",
       "      <td>['anxiety', 'affecting', 'studies', 'lately', ...</td>\n",
       "      <td>anxiety affecting study lately feeling anxious...</td>\n",
       "      <td>anxiety affecting study lately feeling anxious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>This is my recovery story since falling for th...</td>\n",
       "      <td>This is my recovery story since falling for th...</td>\n",
       "      <td>['this', 'is', 'my', 'recovery', 'story', 'sin...</td>\n",
       "      <td>['recovery', 'story', 'since', 'falling', 'pin...</td>\n",
       "      <td>recovery story since falling pin code scam cam...</td>\n",
       "      <td>recovery story since falling pin code scam cam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>After years wearing braces my teeth still look...</td>\n",
       "      <td>After years wearing braces my teeth still look...</td>\n",
       "      <td>['after', 'years', 'wearing', 'braces', 'my', ...</td>\n",
       "      <td>['years', 'wearing', 'braces', 'teeth', 'still...</td>\n",
       "      <td>year wearing brace teeth still look fucked dup...</td>\n",
       "      <td>year wearing brace teeth still look fucked dup...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit                                               post  \\\n",
       "0   Anxiety  Coping We’re moving out of state and away from...   \n",
       "1   Anxiety  Starting a new job anxiety As the title says, ...   \n",
       "2   Anxiety  anxiety affecting studies Lately I've been fee...   \n",
       "3   Anxiety  This is my recovery story since falling for th...   \n",
       "4   Anxiety  After years wearing braces my teeth still look...   \n",
       "\n",
       "                                        post_cleaned  \\\n",
       "0  Coping We’re moving out of state and away from...   \n",
       "1  Starting a new job anxiety As the title says I...   \n",
       "2  anxiety affecting studies Lately Ive been feel...   \n",
       "3  This is my recovery story since falling for th...   \n",
       "4  After years wearing braces my teeth still look...   \n",
       "\n",
       "                                      post_tokenised  \\\n",
       "0  ['coping', 'we', 're', 'moving', 'out', 'of', ...   \n",
       "1  ['starting', 'a', 'new', 'job', 'anxiety', 'as...   \n",
       "2  ['anxiety', 'affecting', 'studies', 'lately', ...   \n",
       "3  ['this', 'is', 'my', 'recovery', 'story', 'sin...   \n",
       "4  ['after', 'years', 'wearing', 'braces', 'my', ...   \n",
       "\n",
       "                                        post_no_stop  \\\n",
       "0  ['coping', 'moving', 'state', 'away', 'everyon...   \n",
       "1  ['starting', 'new', 'job', 'anxiety', 'title',...   \n",
       "2  ['anxiety', 'affecting', 'studies', 'lately', ...   \n",
       "3  ['recovery', 'story', 'since', 'falling', 'pin...   \n",
       "4  ['years', 'wearing', 'braces', 'teeth', 'still...   \n",
       "\n",
       "                                     post_lemmatised  \\\n",
       "0  coping moving state away everyone know sunday ...   \n",
       "1  starting new job anxiety title say starting ne...   \n",
       "2  anxiety affecting study lately feeling anxious...   \n",
       "3  recovery story since falling pin code scam cam...   \n",
       "4  year wearing brace teeth still look fucked dup...   \n",
       "\n",
       "                                         post_string  \n",
       "0  coping moving state away everyone know sunday ...  \n",
       "1  starting new job anxiety title say starting ne...  \n",
       "2  anxiety affecting study lately feeling anxious...  \n",
       "3  recovery story since falling pin code scam cam...  \n",
       "4  year wearing brace teeth still look fucked dup...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post</th>\n",
       "      <th>post_cleaned</th>\n",
       "      <th>post_tokenised</th>\n",
       "      <th>post_no_stop</th>\n",
       "      <th>post_lemmatised</th>\n",
       "      <th>post_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>depression</td>\n",
       "      <td>20,M. My “best years” are being wasted and I b...</td>\n",
       "      <td>M My “best years” are being wasted and I blame...</td>\n",
       "      <td>['m', 'my', 'best', 'years', 'are', 'being', '...</td>\n",
       "      <td>['best', 'years', 'wasted', 'blame', 'depressi...</td>\n",
       "      <td>best year wasted blame depression time see cha...</td>\n",
       "      <td>best year wasted blame depression time see cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>depression</td>\n",
       "      <td>I want to die I just want to die, I dont even ...</td>\n",
       "      <td>I want to die I just want to die I dont even w...</td>\n",
       "      <td>['i', 'want', 'to', 'die', 'i', 'just', 'want'...</td>\n",
       "      <td>['want', 'die', 'want', 'die', 'dont', 'want',...</td>\n",
       "      <td>want die want die dont want talk anymore want ...</td>\n",
       "      <td>want die want die dont want talk anymore want ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>depression</td>\n",
       "      <td>Can someone please just reassure me that it'll...</td>\n",
       "      <td>Can someone please just reassure me that itll ...</td>\n",
       "      <td>['can', 'someone', 'please', 'just', 'reassure...</td>\n",
       "      <td>['someone', 'please', 'reassure', 'itll', 'ok'...</td>\n",
       "      <td>someone please reassure itll ok british girl g...</td>\n",
       "      <td>someone please reassure itll ok british girl g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2472</th>\n",
       "      <td>depression</td>\n",
       "      <td>Tired I remember what it feels like to be happ...</td>\n",
       "      <td>Tired I remember what it feels like to be happ...</td>\n",
       "      <td>['tired', 'i', 'remember', 'what', 'it', 'feel...</td>\n",
       "      <td>['tired', 'remember', 'feels', 'happy', 'actua...</td>\n",
       "      <td>tired remember feel happy actually look forwar...</td>\n",
       "      <td>tired remember feel happy actually look forwar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>depression</td>\n",
       "      <td>Corona has ruined everything I’m almost positi...</td>\n",
       "      <td>Corona has ruined everything I’m almost positi...</td>\n",
       "      <td>['corona', 'has', 'ruined', 'everything', 'i',...</td>\n",
       "      <td>['corona', 'ruined', 'everything', 'almost', '...</td>\n",
       "      <td>corona ruined everything almost positive lost ...</td>\n",
       "      <td>corona ruined everything almost positive lost ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit                                               post  \\\n",
       "2469  depression  20,M. My “best years” are being wasted and I b...   \n",
       "2470  depression  I want to die I just want to die, I dont even ...   \n",
       "2471  depression  Can someone please just reassure me that it'll...   \n",
       "2472  depression  Tired I remember what it feels like to be happ...   \n",
       "2473  depression  Corona has ruined everything I’m almost positi...   \n",
       "\n",
       "                                           post_cleaned  \\\n",
       "2469  M My “best years” are being wasted and I blame...   \n",
       "2470  I want to die I just want to die I dont even w...   \n",
       "2471  Can someone please just reassure me that itll ...   \n",
       "2472  Tired I remember what it feels like to be happ...   \n",
       "2473  Corona has ruined everything I’m almost positi...   \n",
       "\n",
       "                                         post_tokenised  \\\n",
       "2469  ['m', 'my', 'best', 'years', 'are', 'being', '...   \n",
       "2470  ['i', 'want', 'to', 'die', 'i', 'just', 'want'...   \n",
       "2471  ['can', 'someone', 'please', 'just', 'reassure...   \n",
       "2472  ['tired', 'i', 'remember', 'what', 'it', 'feel...   \n",
       "2473  ['corona', 'has', 'ruined', 'everything', 'i',...   \n",
       "\n",
       "                                           post_no_stop  \\\n",
       "2469  ['best', 'years', 'wasted', 'blame', 'depressi...   \n",
       "2470  ['want', 'die', 'want', 'die', 'dont', 'want',...   \n",
       "2471  ['someone', 'please', 'reassure', 'itll', 'ok'...   \n",
       "2472  ['tired', 'remember', 'feels', 'happy', 'actua...   \n",
       "2473  ['corona', 'ruined', 'everything', 'almost', '...   \n",
       "\n",
       "                                        post_lemmatised  \\\n",
       "2469  best year wasted blame depression time see cha...   \n",
       "2470  want die want die dont want talk anymore want ...   \n",
       "2471  someone please reassure itll ok british girl g...   \n",
       "2472  tired remember feel happy actually look forwar...   \n",
       "2473  corona ruined everything almost positive lost ...   \n",
       "\n",
       "                                            post_string  \n",
       "2469  best year wasted blame depression time see cha...  \n",
       "2470  want die want die dont want talk anymore want ...  \n",
       "2471  someone please reassure itll ok british girl g...  \n",
       "2472  tired remember feel happy actually look forwar...  \n",
       "2473  corona ruined everything almost positive lost ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View sample rows of the merged cleaned dataset\n",
    "display(combined_cleaned.head())\n",
    "display(combined_cleaned.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7ad44",
   "metadata": {},
   "source": [
    "### Convert Subreddit Labels to Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47282ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns number value to subreddit name (0 for anxiety, 1 for depression)\n",
    "combined_cleaned['subreddit'] = combined_cleaned['subreddit'].map({'Anxiety': 0, 'depression': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f25c6170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post</th>\n",
       "      <th>post_cleaned</th>\n",
       "      <th>post_tokenised</th>\n",
       "      <th>post_no_stop</th>\n",
       "      <th>post_lemmatised</th>\n",
       "      <th>post_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Coping We’re moving out of state and away from...</td>\n",
       "      <td>Coping We’re moving out of state and away from...</td>\n",
       "      <td>['coping', 'we', 're', 'moving', 'out', 'of', ...</td>\n",
       "      <td>['coping', 'moving', 'state', 'away', 'everyon...</td>\n",
       "      <td>coping moving state away everyone know sunday ...</td>\n",
       "      <td>coping moving state away everyone know sunday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Starting a new job anxiety As the title says, ...</td>\n",
       "      <td>Starting a new job anxiety As the title says I...</td>\n",
       "      <td>['starting', 'a', 'new', 'job', 'anxiety', 'as...</td>\n",
       "      <td>['starting', 'new', 'job', 'anxiety', 'title',...</td>\n",
       "      <td>starting new job anxiety title say starting ne...</td>\n",
       "      <td>starting new job anxiety title say starting ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>anxiety affecting studies Lately I've been fee...</td>\n",
       "      <td>anxiety affecting studies Lately Ive been feel...</td>\n",
       "      <td>['anxiety', 'affecting', 'studies', 'lately', ...</td>\n",
       "      <td>['anxiety', 'affecting', 'studies', 'lately', ...</td>\n",
       "      <td>anxiety affecting study lately feeling anxious...</td>\n",
       "      <td>anxiety affecting study lately feeling anxious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This is my recovery story since falling for th...</td>\n",
       "      <td>This is my recovery story since falling for th...</td>\n",
       "      <td>['this', 'is', 'my', 'recovery', 'story', 'sin...</td>\n",
       "      <td>['recovery', 'story', 'since', 'falling', 'pin...</td>\n",
       "      <td>recovery story since falling pin code scam cam...</td>\n",
       "      <td>recovery story since falling pin code scam cam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>After years wearing braces my teeth still look...</td>\n",
       "      <td>After years wearing braces my teeth still look...</td>\n",
       "      <td>['after', 'years', 'wearing', 'braces', 'my', ...</td>\n",
       "      <td>['years', 'wearing', 'braces', 'teeth', 'still...</td>\n",
       "      <td>year wearing brace teeth still look fucked dup...</td>\n",
       "      <td>year wearing brace teeth still look fucked dup...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                               post  \\\n",
       "0          0  Coping We’re moving out of state and away from...   \n",
       "1          0  Starting a new job anxiety As the title says, ...   \n",
       "2          0  anxiety affecting studies Lately I've been fee...   \n",
       "3          0  This is my recovery story since falling for th...   \n",
       "4          0  After years wearing braces my teeth still look...   \n",
       "\n",
       "                                        post_cleaned  \\\n",
       "0  Coping We’re moving out of state and away from...   \n",
       "1  Starting a new job anxiety As the title says I...   \n",
       "2  anxiety affecting studies Lately Ive been feel...   \n",
       "3  This is my recovery story since falling for th...   \n",
       "4  After years wearing braces my teeth still look...   \n",
       "\n",
       "                                      post_tokenised  \\\n",
       "0  ['coping', 'we', 're', 'moving', 'out', 'of', ...   \n",
       "1  ['starting', 'a', 'new', 'job', 'anxiety', 'as...   \n",
       "2  ['anxiety', 'affecting', 'studies', 'lately', ...   \n",
       "3  ['this', 'is', 'my', 'recovery', 'story', 'sin...   \n",
       "4  ['after', 'years', 'wearing', 'braces', 'my', ...   \n",
       "\n",
       "                                        post_no_stop  \\\n",
       "0  ['coping', 'moving', 'state', 'away', 'everyon...   \n",
       "1  ['starting', 'new', 'job', 'anxiety', 'title',...   \n",
       "2  ['anxiety', 'affecting', 'studies', 'lately', ...   \n",
       "3  ['recovery', 'story', 'since', 'falling', 'pin...   \n",
       "4  ['years', 'wearing', 'braces', 'teeth', 'still...   \n",
       "\n",
       "                                     post_lemmatised  \\\n",
       "0  coping moving state away everyone know sunday ...   \n",
       "1  starting new job anxiety title say starting ne...   \n",
       "2  anxiety affecting study lately feeling anxious...   \n",
       "3  recovery story since falling pin code scam cam...   \n",
       "4  year wearing brace teeth still look fucked dup...   \n",
       "\n",
       "                                         post_string  \n",
       "0  coping moving state away everyone know sunday ...  \n",
       "1  starting new job anxiety title say starting ne...  \n",
       "2  anxiety affecting study lately feeling anxious...  \n",
       "3  recovery story since falling pin code scam cam...  \n",
       "4  year wearing brace teeth still look fucked dup...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post</th>\n",
       "      <th>post_cleaned</th>\n",
       "      <th>post_tokenised</th>\n",
       "      <th>post_no_stop</th>\n",
       "      <th>post_lemmatised</th>\n",
       "      <th>post_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>1</td>\n",
       "      <td>20,M. My “best years” are being wasted and I b...</td>\n",
       "      <td>M My “best years” are being wasted and I blame...</td>\n",
       "      <td>['m', 'my', 'best', 'years', 'are', 'being', '...</td>\n",
       "      <td>['best', 'years', 'wasted', 'blame', 'depressi...</td>\n",
       "      <td>best year wasted blame depression time see cha...</td>\n",
       "      <td>best year wasted blame depression time see cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>1</td>\n",
       "      <td>I want to die I just want to die, I dont even ...</td>\n",
       "      <td>I want to die I just want to die I dont even w...</td>\n",
       "      <td>['i', 'want', 'to', 'die', 'i', 'just', 'want'...</td>\n",
       "      <td>['want', 'die', 'want', 'die', 'dont', 'want',...</td>\n",
       "      <td>want die want die dont want talk anymore want ...</td>\n",
       "      <td>want die want die dont want talk anymore want ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>1</td>\n",
       "      <td>Can someone please just reassure me that it'll...</td>\n",
       "      <td>Can someone please just reassure me that itll ...</td>\n",
       "      <td>['can', 'someone', 'please', 'just', 'reassure...</td>\n",
       "      <td>['someone', 'please', 'reassure', 'itll', 'ok'...</td>\n",
       "      <td>someone please reassure itll ok british girl g...</td>\n",
       "      <td>someone please reassure itll ok british girl g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2472</th>\n",
       "      <td>1</td>\n",
       "      <td>Tired I remember what it feels like to be happ...</td>\n",
       "      <td>Tired I remember what it feels like to be happ...</td>\n",
       "      <td>['tired', 'i', 'remember', 'what', 'it', 'feel...</td>\n",
       "      <td>['tired', 'remember', 'feels', 'happy', 'actua...</td>\n",
       "      <td>tired remember feel happy actually look forwar...</td>\n",
       "      <td>tired remember feel happy actually look forwar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>1</td>\n",
       "      <td>Corona has ruined everything I’m almost positi...</td>\n",
       "      <td>Corona has ruined everything I’m almost positi...</td>\n",
       "      <td>['corona', 'has', 'ruined', 'everything', 'i',...</td>\n",
       "      <td>['corona', 'ruined', 'everything', 'almost', '...</td>\n",
       "      <td>corona ruined everything almost positive lost ...</td>\n",
       "      <td>corona ruined everything almost positive lost ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit                                               post  \\\n",
       "2469          1  20,M. My “best years” are being wasted and I b...   \n",
       "2470          1  I want to die I just want to die, I dont even ...   \n",
       "2471          1  Can someone please just reassure me that it'll...   \n",
       "2472          1  Tired I remember what it feels like to be happ...   \n",
       "2473          1  Corona has ruined everything I’m almost positi...   \n",
       "\n",
       "                                           post_cleaned  \\\n",
       "2469  M My “best years” are being wasted and I blame...   \n",
       "2470  I want to die I just want to die I dont even w...   \n",
       "2471  Can someone please just reassure me that itll ...   \n",
       "2472  Tired I remember what it feels like to be happ...   \n",
       "2473  Corona has ruined everything I’m almost positi...   \n",
       "\n",
       "                                         post_tokenised  \\\n",
       "2469  ['m', 'my', 'best', 'years', 'are', 'being', '...   \n",
       "2470  ['i', 'want', 'to', 'die', 'i', 'just', 'want'...   \n",
       "2471  ['can', 'someone', 'please', 'just', 'reassure...   \n",
       "2472  ['tired', 'i', 'remember', 'what', 'it', 'feel...   \n",
       "2473  ['corona', 'has', 'ruined', 'everything', 'i',...   \n",
       "\n",
       "                                           post_no_stop  \\\n",
       "2469  ['best', 'years', 'wasted', 'blame', 'depressi...   \n",
       "2470  ['want', 'die', 'want', 'die', 'dont', 'want',...   \n",
       "2471  ['someone', 'please', 'reassure', 'itll', 'ok'...   \n",
       "2472  ['tired', 'remember', 'feels', 'happy', 'actua...   \n",
       "2473  ['corona', 'ruined', 'everything', 'almost', '...   \n",
       "\n",
       "                                        post_lemmatised  \\\n",
       "2469  best year wasted blame depression time see cha...   \n",
       "2470  want die want die dont want talk anymore want ...   \n",
       "2471  someone please reassure itll ok british girl g...   \n",
       "2472  tired remember feel happy actually look forwar...   \n",
       "2473  corona ruined everything almost positive lost ...   \n",
       "\n",
       "                                            post_string  \n",
       "2469  best year wasted blame depression time see cha...  \n",
       "2470  want die want die dont want talk anymore want ...  \n",
       "2471  someone please reassure itll ok british girl g...  \n",
       "2472  tired remember feel happy actually look forwar...  \n",
       "2473  corona ruined everything almost positive lost ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(combined_cleaned.head())\n",
    "display(combined_cleaned.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566cd221",
   "metadata": {},
   "source": [
    "### Define X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3c3628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign x and y values\n",
    "X = combined_cleaned['post_cleaned']\n",
    "y = combined_cleaned['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a659f32",
   "metadata": {},
   "source": [
    "### Baseline score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b435ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.536444\n",
       "1    0.463556\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensures the class is balanced\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "185b5f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Score: 0.5364436949597152\n"
     ]
    }
   ],
   "source": [
    "baseline_score = y.value_counts(normalize = True)\n",
    "print(f'Baseline Score: {baseline_score[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2d7c93",
   "metadata": {},
   "source": [
    "### Do train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc75f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    stratify=y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2036f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (4002,)\n",
      "X_test (1335,)\n"
     ]
    }
   ],
   "source": [
    "# View how many rows are in train set and test set respectively\n",
    "print('X_train', X_train.shape)\n",
    "print('X_test', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d69c9984",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(X_train, y_train, left_index = True, right_index = True)\n",
    "\n",
    "test = pd.merge(X_test, y_test, left_index = True, right_index = True)\n",
    "\n",
    "train.to_csv(\"./data/train.csv\", index = False)\n",
    "test.to_csv(\"./data/test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce95f29",
   "metadata": {},
   "source": [
    "### Vectorise words with CountVectoriser and TF-IDF Vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69a6bb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_cvec (4002, 17245)\n",
      "X_test_cvec (1335, 17245)\n",
      "X_train_tvec (4002, 17245)\n",
      "X_test_tvec (1335, 17245)\n"
     ]
    }
   ],
   "source": [
    "cvec = CountVectorizer()\n",
    "\n",
    "X_train_cvec = cvec.fit_transform(X_train)\n",
    "X_test_cvec = cvec.transform(X_test)\n",
    "\n",
    "print('X_train_cvec', X_train_cvec.shape)\n",
    "print('X_test_cvec', X_test_cvec.shape)\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "\n",
    "X_train_tvec = tvec.fit_transform(X_train)\n",
    "X_test_tvec = tvec.transform(X_test)\n",
    "\n",
    "print('X_train_tvec', X_train_tvec.shape)\n",
    "print('X_test_tvec', X_test_tvec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b9014",
   "metadata": {},
   "source": [
    "### Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1b5a034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0.8528202247191011\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "print('Train', cross_val_score(nb, X_train_cvec, y_train, cv=5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b7da214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0.859322097378277\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=200)\n",
    "print('Train', cross_val_score(lr, X_train_cvec, y_train, cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f613f4",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da537f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer hyperparameters involving ngram, stop word removal, and feature numbers\n",
    "vec_params_features = {\n",
    "    \"vec__ngram_range\": [(1, 1), (1, 2), (1, 3)], \n",
    "    \"vec__stop_words\": [None, \"english\"], \n",
    "    \"vec__max_features\": [100, 300, 500, 700, 900]}\n",
    "\n",
    "# vectorizer hyperparameters involving ngram, stop word removal, min document appearance, and max document appearance\n",
    "vec_params_dfs = {\n",
    "    \"vec__ngram_range\": [(1, 1), (1, 2), (1, 3)], \n",
    "    \"vec__stop_words\": [None, \"english\"], \n",
    "    \"vec__min_df\": [0.1, 0.2, 0.3], \n",
    "    \"vec__max_df\": [0.6, 0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# vectorizer hyperparameters involving all hyperparameters as above, though with more limited options \n",
    "vec_params_all = {\n",
    "    \"vec__ngram_range\": [(1, 1), (1, 2)], \n",
    "    \"vec__stop_words\": [None, \"english\"], \n",
    "    \"vec__min_df\": [0.1, 0.2], \n",
    "    \"vec__max_df\": [0.7, 0.8], \n",
    "    \"vec__max_features\": [300, 500, 700]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7eee0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store model metrics in \n",
    "# will be transformed to DataFrame at end for easy visualization of performance differences\n",
    "model_outcomes = {\"Transformer\": [], \n",
    "                  \"Estimator\": [], \n",
    "                  \"Parameters\": [],\n",
    "                  \"Best Parameters\": [], \n",
    "                  \"Best Score\": [], \n",
    "                  \"Training Score\": [], \n",
    "                  \"Test Score\": [], \n",
    "                  \"Discrepancy\": [], \n",
    "                  \"Runtime\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abd3bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\"df_params\": vec_params_dfs, \"features_params\": vec_params_features, \n",
    "              \"limited_all_params\": vec_params_all}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16db12e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(start, end): \n",
    "    long = end - start \n",
    "    minutes = int(long // 60 )\n",
    "    seconds = int(round(long - 60 * minutes))\n",
    "    return f\"{minutes}m {seconds}s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0f0d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch_batch(vectorizer, classifier, parameter_dict, outcomes_dict): \n",
    "    parameter_names = list(param_dict.keys())\n",
    "    \n",
    "    cycle = 0\n",
    "    \n",
    "    time_total = 0 \n",
    "    for i in param_dict:\n",
    "        pipe = Pipeline([\n",
    "            (\"vec\", vectorizer), \n",
    "            (\"class\", classifier)\n",
    "        ])\n",
    "        \n",
    "        grid = GridSearchCV(pipe, parameter_dict[i], cv = 5)\n",
    "        \n",
    "        start = time.time() \n",
    "        grid.fit(X_train, y_train)\n",
    "        end = time.time()\n",
    "        \n",
    "        train = grid.score(X_train, y_train)\n",
    "        test = grid.score(X_test, y_test)\n",
    "        \n",
    "        print(f\"Model with {parameter_names[cycle]} took {run(start, end)} to run.\")\n",
    "        print(f\"Best parameters: \\n{grid.best_params_}\")\n",
    "        print(f\"Best score: {grid.best_score_}\")\n",
    "        print(f\"Training score: {train}\")\n",
    "        print(f\"Test score: {test}\")\n",
    "        \n",
    "        fill = [f\"{vectorizer}\", f\"{classifier}\", parameter_names[cycle], grid.best_params_, grid.best_score_, \n",
    "                train, test, (train - test), run(start, end)]\n",
    "        \n",
    "        count = 0\n",
    "        for field in outcomes_dict: \n",
    "            outcomes_dict[field].append(fill[count])\n",
    "            count += 1\n",
    "        \n",
    "        print(\"----------\")\n",
    "        \n",
    "        cycle += 1\n",
    "        time_total += (end - start)\n",
    "    print(f\"This entire process took {run(0, time_total)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c511bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Count Vectoriser with Logistic Regression\n",
    "gridsearch_batch(CountVectorizer(), LogisticRegression(solver='liblinear', class_weight='balanced'), param_dict, model_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fe75a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Count Vectoriser with Multinomial Naive-Bayes\n",
    "gridsearch_batch(CountVectorizer(), MultinomialNB(), param_dict, model_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54c3284",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF-IDF Vectoriser with Logistic Regression\n",
    "\n",
    "gridsearch_batch(TfidfVectorizer(), LogisticRegression(solver='liblinear', class_weight='balanced'), param_dict, model_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d9f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF-IDF Vectoriser with Multinomial Naive-Bayes\n",
    "gridsearch_batch(TfidfVectorizer(), MultinomialNB(), param_dict, model_outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b79cb",
   "metadata": {},
   "source": [
    "### Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1191e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = pd.DataFrame(model_outcomes)\n",
    "\n",
    "outcomes.sort_values(by = \"Best Score\", ascending = False, inplace = True)\n",
    "\n",
    "outcomes.reset_index(inplace = True)\n",
    "\n",
    "outcomes.drop(columns = [\"index\"], inplace = True)\n",
    "\n",
    "outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf4e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling best parameters for the model that yielded a high accuracy score (91%) with a smaller discrepancy (2.2%)\n",
    "\n",
    "outcomes[\"Best Parameters\"][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2922c4",
   "metadata": {},
   "source": [
    "### Run Finalised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a946fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "                (\"vec\", TfidfVectorizer(max_features = 900, \n",
    "                                        ngram_range = (1, 3), \n",
    "                                        stop_words = \"english\")),\n",
    "                (\"lr\", LogisticRegression(solver = \"liblinear\"))\n",
    "            ])\n",
    "\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf9a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b556ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build pipeline\n",
    "  ##count vec params unigrams, bigrams, min/max df\n",
    "  ## model params\n",
    "#gridsearch cv (what kind of regularisation i want to do)\n",
    "  #adjust regularisation param\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1247653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46271762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "# simplest is accuracy--> only for data we have (TP+TN/all)\n",
    "# recall -minimise errors TP/(TP+FN) if FN is 0 then recall =1\n",
    "# take note which are the important tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8897f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aoc .5 -1\n",
    "#worst =.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a2ee94",
   "metadata": {},
   "source": [
    "# Evaluation and Conceptual Understanding\n",
    "\n",
    "- Does the student accurately identify and explain the baseline score?\n",
    "- Does the student select and use metrics relevant to the problem objective?\n",
    "- Does the student interpret the results of their model for purposes of inference?\n",
    "- Is domain knowledge demonstrated when interpreting results?\n",
    "- Does the student provide appropriate interpretation with regards to descriptive and inferential statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7c6f22",
   "metadata": {},
   "source": [
    "# Conclusion and Recommendations\n",
    "\n",
    "- Does the student provide appropriate context to connect individual steps back to the overall project?\n",
    "- Is it clear how the final recommendations were reached?\n",
    "- Are the conclusions/recommendations clearly stated?\n",
    "- Does the conclusion answer the original problem statement?\n",
    "- Does the student address how findings of this research can be applied for the benefit of stakeholders?\n",
    "- Are future steps to move the project forward identified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8baa273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
